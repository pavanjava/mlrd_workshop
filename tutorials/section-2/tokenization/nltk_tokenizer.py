from nltk import word_tokenize

text = "I Love Data Science and Artificial Intelligence."

tokens = word_tokenize(text)

print(tokens)